# L=100 CPU 초기 실험 설정

L=100km 거리에서 CPU를 사용한 초기 모델 실험. MLP와 FT-Transformer의 다양한 변형을 비교 테스트.

## 공통 설정

```python
COMMON_CONFIG = {
    # 기본 파라미터
    'L': 100,                     # 거리 (km)
    'epochs': 200,                # 훈련 에포크 수
    'batch_size': 128,            # 배치 크기
    'device': 'cpu',              # 디바이스 (CPU 전용)
    
    # 최적화 설정
    'optimizer': 'Adam',          # 옵티마이저
    'learning_rate': 0.001,       # 학습률
    'weight_decay': 1e-5,         # 가중치 감쇠
    'dropout_rate': 0.1,          # 드롭아웃 비율
    
    # 정규화
    'scaler': 'MinMaxScaler',     # 입력/출력 모두 MinMaxScaler
    'gradient_clipping': 1.0,     # Gradient clipping max_norm
    'activation': 'ReLU + Sigmoid'  # 은닉층: ReLU, 출력층: Sigmoid
}
```

## 아키텍처 설정

### MLP
```python
{
    'hidden_sizes': [512, 256],   # 2개 은닉층
    'input_size': 8,              # 입력 변수 (Y_0 포함)
    'output_size': 9              # 8개 파라미터 + SKR
}
```

### FT-Transformer
```python
{
    'd_embed': 32,                # 임베딩 차원
    'n_heads': 4,                 # Attention head 수
    'n_layers': 3,                # Transformer layer 수
    'dim_feedforward': 128,       # Feedforward 차원
    'input_size': 8,              # 입력 변수 (Y_0 포함)
    'output_size': 9              # 8개 파라미터 + SKR
}
```

## 실험 옵션 설명

| 옵션 | 설명 |
|------|------|
| `schedule` | ReduceLROnPlateau 스케줄러 (patience=10, factor=0.5) |
| `norm` | p_mu, p_nu, p_vac 정규화 (합=1) |
| `sameweight` | 모든 파라미터 가중치 1로 동일 (기본: SKR=1000배) |
| `cls` | CLS Token 사용 (전체 정보 집약) |
| `flatten` | Flatten 방식 (토큰별 출력 평균) |

---

## MLP 모델

### basic_mlp.pth

**기본 MLP 모델**

```python
{
    'model': 'MLP',
    'hidden_sizes': [512, 256],
    'loss_weight': [1, 1, 1, 1, 1, 1, 1, 1, 1000]  # SKR=1000x
}
```

---

### mlp_schedule_norm.pth

**변경사항:** Scheduler + 정규화

```python
{
    'model': 'MLP',
    'hidden_sizes': [512, 256],
    'scheduler': 'ReduceLROnPlateau',  # 추가
    'p_normalization': True,           # 추가
    'loss_weight': [1, 1, 1, 1, 1, 1, 1, 1, 1000]
}
```

---

## FT-Transformer (CLS Token)

### cls_ft.pth

**기본 FT-Transformer (CLS Token)**

```python
{
    'model': 'FT-Transformer',
    'd_embed': 32,
    'n_heads': 4,
    'n_layers': 3,
    'dim_feedforward': 128,
    'pooling': 'cls',                  # CLS Token 사용
    'loss_weight': [1, 1, 1, 1, 1, 1, 1, 1, 1000]
}
```

---

### cls_ft_schedule.pth

**변경사항:** Scheduler 추가

```python
{
    'model': 'FT-Transformer',
    'd_embed': 32,
    'n_heads': 4,
    'n_layers': 3,
    'dim_feedforward': 128,
    'pooling': 'cls',
    'scheduler': 'ReduceLROnPlateau',  # 추가
    'loss_weight': [1, 1, 1, 1, 1, 1, 1, 1, 1000]
}
```

---

### cls_ft_norm.pth

**변경사항:** 정규화 추가

```python
{
    'model': 'FT-Transformer',
    'd_embed': 32,
    'n_heads': 4,
    'n_layers': 3,
    'dim_feedforward': 128,
    'pooling': 'cls',
    'p_normalization': True,           # 추가
    'loss_weight': [1, 1, 1, 1, 1, 1, 1, 1, 1000]
}
```

---

### cls_ft_schedule_norm.pth

**변경사항:** Scheduler + 정규화

```python
{
    'model': 'FT-Transformer',
    'd_embed': 32,
    'n_heads': 4,
    'n_layers': 3,
    'dim_feedforward': 128,
    'pooling': 'cls',
    'scheduler': 'ReduceLROnPlateau',  # 추가
    'p_normalization': True,           # 추가
    'loss_weight': [1, 1, 1, 1, 1, 1, 1, 1, 1000]
}
```

---

### cls_ft_schedule_norm_sameweight.pth

**변경사항:** Scheduler + 정규화 + 동일 가중치

```python
{
    'model': 'FT-Transformer',
    'd_embed': 32,
    'n_heads': 4,
    'n_layers': 3,
    'dim_feedforward': 128,
    'pooling': 'cls',
    'scheduler': 'ReduceLROnPlateau',  # 추가
    'p_normalization': True,           # 추가
    'loss_weight': [1, 1, 1, 1, 1, 1, 1, 1, 1]  # 모두 1로 동일
}
```

---

## FT-Transformer (Flatten)

### flatten_ft.pth

**기본 FT-Transformer (Flatten)**

```python
{
    'model': 'FT-Transformer',
    'd_embed': 32,
    'n_heads': 4,
    'n_layers': 3,
    'dim_feedforward': 128,
    'pooling': 'flatten',              # Flatten 방식
    'loss_weight': [1, 1, 1, 1, 1, 1, 1, 1, 1000]
}
```

---

### flatten_ft_schedule_norm.pth

**변경사항:** Scheduler + 정규화

```python
{
    'model': 'FT-Transformer',
    'd_embed': 32,
    'n_heads': 4,
    'n_layers': 3,
    'dim_feedforward': 128,
    'pooling': 'flatten',
    'scheduler': 'ReduceLROnPlateau',  # 추가
    'p_normalization': True,           # 추가
    'loss_weight': [1, 1, 1, 1, 1, 1, 1, 1, 1000]
}
```

---

## 실험 목적

초기 CPU 환경에서 다양한 모델 구조와 학습 기법을 비교하여 최적 조합 탐색:
- **MLP vs FT-Transformer**: 아키텍처 비교
- **CLS vs Flatten**: Pooling 방식 비교
- **Scheduler 효과**: LR 동적 조정 효과 검증
- **정규화 효과**: p 파라미터 정규화 효과 검증
- **가중치 전략**: SKR 집중 vs 균등 가중치 비교

**결론:** CLS Token + Scheduler + 정규화 조합이 최적 성능 (이후 GPU 실험으로 확장)
