# L=100 GPU 세부 튜닝 설정

GPU 아키텍처 실험에서 검증된 최적 구조(`d_embed=128`, `n_layers=4`, `dim_feedforward=256`)를 기반으로 배치 크기, 손실 가중치, 정규화 파라미터 세부 튜닝.

## 공통 설정

```python
COMMON_CONFIG = {
    # 기본 파라미터
    'L': 100,                         # 거리 (km)
    'epochs': 200,                    # 훈련 에포크 수
    'device': 'cuda',                 # 디바이스 (GPU 전용)
    
    # 최적화 설정 (gpu6 기반)
    'optimizer': 'Adam',              # 옵티마이저
    'learning_rate': 0.0005,          # 학습률
    'dropout_rate': 0.1,              # 드롭아웃 비율
    
    # 최적 아키텍처 (GPU 실험에서 검증)
    'd_embed': 128,                   # 임베딩 차원
    'n_heads': 4,                     # Attention head 수
    'n_layers': 4,                    # Transformer 레이어 수
    'dim_feedforward': 256,           # Feedforward 차원
    'input_size': 8,                  # 입력 변수 (Y_0 포함)
    'output_size': 9,                 # 8개 파라미터 + SKR
    
    # 학습 기법
    'scheduler': 'ReduceLROnPlateau', # LR 스케줄러
    'scheduler_patience': 10,         # 개선 없을 시 LR 감소 대기 에포크
    'scheduler_factor': 0.5,          # LR 감소 비율
    'p_normalization': True,          # p_mu, p_nu, p_vac 정규화 (합=1)
    'pooling': 'cls',                 # CLS Token 사용
    
    # Early Stopping
    'early_stopping': True,           # Early stopping 사용
    'early_stopping_patience': 30,    # 중단 전 대기 에포크
    'early_stopping_min_delta': 1e-6, # 개선 최소 변화량
    
    # 정규화 및 손실 함수
    'scaler': 'MinMaxScaler',         # 입력/출력 모두 MinMaxScaler
    'gradient_clipping': 1.0,         # Gradient clipping max_norm
    'activation': 'ReLU + Sigmoid'    # 은닉층: ReLU, 출력층: Sigmoid
}
```

## 실험 변수

GPU 아키텍처가 고정된 상태에서 **학습 하이퍼파라미터**를 점진적으로 조정:

| 변수 | 설명 | 실험 범위 |
|------|------|----------|
| `MSE SKR weight` | MSE 손실에서 SKR의 가중치 | 1000 → 100 |
| `batch_size` | 배치 크기 | 128 → 64 |
| `weight_decay` | 가중치 감쇠 | 1e-5 → 5e-6 |

---

## 모델별 설정

### cls_ft_schedule_norm_100skr.pth

**변경사항:** SKR 가중치 재조정 (1배 → 100배)

```python
{
    'model': 'FT-Transformer',
    'device': 'cuda',
    
    # 아키텍처 (gpu6와 동일)
    'd_embed': 128,
    'n_heads': 4,
    'n_layers': 4,
    'dim_feedforward': 256,
    
    # 학습 설정
    'batch_size': 128,                # 기본 배치 크기 유지
    'learning_rate': 0.0005,
    'weight_decay': 1e-5,             # 기본 weight decay 유지
    
    # MSE 손실 가중치
    'loss_weight': [1, 1, 1, 1, 1, 1, 1, 1, 1000] → [1, 1, 1, 1, 1, 1, 1, 1, 100],  # 🔹 SKR 가중치 1000배에서 100배로 감소
    
    'pooling': 'cls',
    'scheduler': 'ReduceLROnPlateau',
    'p_normalization': True
}
```

**실험 목적:** gpu6까지 SKR 가중치는 1000배였으나, 파라미터 예측 균형을 위해 100배로 감소 (loss_scaling은 gpu6처럼 1 유지)

---

### cls_ft_schedule_norm_100skr_64bs.pth

**변경사항:** 배치 크기 감소 (128 → 64)

```python
{
    # ... (공통 설정 동일)
    
    # 아키텍처 (동일)
    'd_embed': 128,
    'n_heads': 4,
    'n_layers': 4,
    'dim_feedforward': 256,
    
    # 학습 설정 변경
    'batch_size': 128 → 64,           # 🔹 배치 크기 절반으로 감소
    'learning_rate': 0.0005,
    'weight_decay': 1e-5,
    'loss_weight': [1, 1, 1, 1, 1, 1, 1, 1, 100]
}
```

**실험 목적:** 작은 배치 크기로 gradient 업데이트 빈도 증가, 일반화 성능 개선 시도

---

### cls_ft_schedule_norm_100skr_64bs_5e-6wd.pth

**변경사항:** Weight Decay 감소 (1e-5 → 5e-6)

```python
{
    # ... (공통 설정 동일)
    
    # 아키텍처 (동일)
    'd_embed': 128,
    'n_heads': 4,
    'n_layers': 4,
    'dim_feedforward': 256,
    
    # 학습 설정 변경
    'batch_size': 64,
    'learning_rate': 0.0005,
    'weight_decay': 1e-5 → 5e-6,      # 🔹 정규화 강도 절반으로 감소
    'loss_weight': [1, 1, 1, 1, 1, 1, 1, 1, 100]
}
```

**실험 목적:** Weight decay를 완화하여 모델 표현력 증가, 과적합 위험 감소와 성능 향상 균형 탐색

---

## 실험 전략

### 단계별 튜닝 순서

1. **SKR 가중치 감소**: 기존 1000배에서 100배로 감소 (loss_scaling은 gpu6의 1 유지)
   - 목표: 파라미터 예측 균형 개선 (SKR 집중도 완화)

2. **배치 크기 감소**: 128 → 64
   - 목표: Gradient 업데이트 빈도 증가, 일반화 성능 개선
   - Trade-off: 학습 시간 증가, 배치별 분산 증가

3. **정규화 완화**: weight_decay 1e-5 → 5e-6
   - 목표: 모델 표현력 증가, 언더피팅 방지
   - Trade-off: 과적합 위험 약간 증가

## 주요 발견

1. **SKR 가중치**: 1000배에서 100배로 감소 시 파라미터 예측 균형 크게 개선, SKR 예측 정확도는 소폭 감소하나 허용 가능
2. **배치 크기 효과**: 64로 감소 시 일반화 성능 소폭 향상, 학습 시간 약 1.5배 증가
3. **Weight Decay**: 5e-6으로 완화 시 복잡한 패턴 학습 개선, 과적합은 Early Stopping으로 방지
4. **최종 조합**: 세 가지 변경을 모두 적용한 `64bs_5e-6wd` 모델이 최적 성능

## 이전 단계와의 비교

| 단계 | 목적 | 주요 변수 |
|------|------|----------|
| **CPU 초기실험** | 학습 기법 탐색 | Scheduler, 정규화, Pooling 방식 |
| **GPU 아키텍처실험** | 모델 크기 최적화 | d_embed, n_layers, dim_feedforward |
| **GPU 세부튜닝** ⭐ | 학습 효율 개선 | batch_size, weight_decay, SKR weight |

## 다음 단계

- 최적 설정(`d_embed=128`, `n_layers=4`, `batch_size=64`, `weight_decay=5e-6`, `SKR weight=100`)을 L=20, L=50에 적용
- 데이터 증강 기법 실험
- Attention 시각화 및 해석
