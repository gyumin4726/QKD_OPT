# L=20 모델 학습 설정

## 공통 설정

```python
# 모든 모델에 공통으로 적용된 설정
COMMON_CONFIG = {
    # 기본 파라미터
    'L': 20,                      # 거리 (km)
    'epochs': 500,                # 훈련 에포크 수
    'batch_size': 64,             # 배치 크기
    'device': 'cuda',             # 디바이스: 'cpu', 'cuda', 'auto'
    
    # Learning Rate Scheduler
    'scheduler_patience': 10,     # LR 감소 전 대기 에포크
    'scheduler_factor': 0.5,      # LR 감소 비율
    
    # Early Stopping
    'early_stopping': True,       # Early stopping 사용 여부
    'early_stopping_patience': 30,  # 조기 종료 대기 에포크
    'early_stopping_min_delta': 1e-6,  # 개선 최소 임계값
    
    # FT-Transformer 아키텍처
    'd_embed': 128,               # 임베딩 차원
    'n_heads': 4,                 # Attention head 수
    'n_layers': 4,                # Transformer layer 수
    'dim_feedforward': 256        # Feedforward 차원
}

# 손실 함수 가중치
param_weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 100.0])
# [mu, nu, vac, p_mu, p_nu, p_vac, p_X, q_X, skr]
```

---

## L_20.pth (기본 모델)

```python
{
    'optimizer': 'Adam',
    'learning_rate': 0.0005,
    'weight_decay': 1e-5,
    'dropout_rate': 0.1,
    'loss_scaling': 1
}
```

---

## L_20_AdamW.pth

**변경사항:** Optimizer 변경

```python
{
    'optimizer': 'AdamW',          # Adam → AdamW
    'learning_rate': 0.0005,
    'weight_decay': 1e-5,
    'dropout_rate': 0.1,
    'loss_scaling': 1
}
```

---

## L_20_5e-6wd.pth

**변경사항:** Weight decay 감소

```python
{
    'optimizer': 'Adam',
    'learning_rate': 0.0005,
    'weight_decay': 5e-6,          # 1e-5 → 5e-6
    'dropout_rate': 0.1,
    'loss_scaling': 1
}
```

---

## L_20_CPU.pth

**변경사항:** CPU에서 학습

```python
{
    'device': 'cpu',               # cuda → cpu
    'optimizer': 'Adam',
    'learning_rate': 0.0005,
    'weight_decay': 5e-6,
    'dropout_rate': 0.1,
    'loss_scaling': 1
}
```